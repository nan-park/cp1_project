{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL82MDGdtREiWbSxtuYkaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nan-park/cp1_project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0과 1을 분류하는 인공신경망 프로그래밍**"
      ],
      "metadata": {
        "id": "2K4TxCWwuRNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import**"
      ],
      "metadata": {
        "id": "3lAvzFhkmi4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (체크) vscode에 주석 달았던 것들 나중에 colab에도 옮겨놓기"
      ],
      "metadata": {
        "id": "fnhdgXKOvEJ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TZZbeiymmbbs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Load**"
      ],
      "metadata": {
        "id": "hTwHDU2wuVyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # google drive mount\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHH52vxqSr2l",
        "outputId": "07732e86-7586-4ff4-e89b-e0c77a34e0ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "def data_load():\n",
        "  df = pd.read_csv('/content/drive/MyDrive/binary_dataset.csv')\n",
        "  return df\n",
        "# df = data_load()"
      ],
      "metadata": {
        "id": "0ARZrNC-mq75"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()"
      ],
      "metadata": {
        "id": "2htdCuKloOKi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Split**"
      ],
      "metadata": {
        "id": "SxW1RzSuudd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minMaxScaler(X, x_max, x_min):\n",
        "  X = X.copy()\n",
        "  for feature in x_max.keys():\n",
        "    X[feature] = (X[feature] - x_min[feature]) / (x_max[feature] - x_min[feature])\n",
        "  return X\n",
        "\n",
        "def normalization(X_train, X_test):\n",
        "  features = list(X_train.columns)\n",
        "  x_max = {}\n",
        "  x_min = {}\n",
        "  for feature in features:\n",
        "    x_max[feature] = X_train[feature].max()\n",
        "    x_min[feature] = X_train[feature].min()\n",
        "  X_train = minMaxScaler(X_train, x_max, x_min)\n",
        "  X_test = minMaxScaler(X_test, x_max, x_min)\n",
        "  return X_train, X_test"
      ],
      "metadata": {
        "id": "GtXGFOScB4_S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습/테스트 데이터 뒤섞기\n",
        "def df_shuffle(df):\n",
        "  return df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# 특성, 타겟 나누기\n",
        "def divide_xy(df):\n",
        "  target = 'y'\n",
        "  features = list(df.columns)\n",
        "  features.remove(target)\n",
        "\n",
        "  X = df[features]\n",
        "  y = np.array(df[target]).reshape(-1, 1)\n",
        "  return X, y\n",
        "\n",
        "# 학습/테스트 데이터 분리하기\n",
        "def train_test_divide(X, y, test_size=0.2):  # X: pandas dataframe, y: numpy array\n",
        "  length = len(y)\n",
        "  test_index = int(length * test_size)\n",
        "\n",
        "  X_test = X[:test_index]\n",
        "  y_test = y[:test_index]\n",
        "\n",
        "  X_train = X[test_index:]  \n",
        "  y_train = y[test_index:]\n",
        "  \n",
        "  # train 데이터에 맞춰서 normalization(minMaxScaler 이용)\n",
        "  X_train, X_test = normalization(X_train, X_test)\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "qxfruCayo0eh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 함수 통합\n",
        "def train_test_split(df, shuffle=True, test_size=0.2):\n",
        "  if shuffle:\n",
        "    df = df_shuffle(df)\n",
        "  \n",
        "  X, y = divide_xy(df)\n",
        "  return train_test_divide(X, y, test_size=test_size)\n",
        "\n",
        "# X_train, y_train, X_test, y_test = train_test_split(df)"
      ],
      "metadata": {
        "id": "EndvpVrA-s4M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 미니배치 설정\n",
        "def split_mini_batch(X, y, n): # train, test 들어올 예정\n",
        "  # 4개씩 미니배치 설정. 나머지는 버리기\n",
        "  length = len(y)\n",
        "  num = length // n # 미니배치 개수\n",
        "  X_batch_list = []\n",
        "  y_batch_list = []\n",
        "  for i in range(num):\n",
        "    i = i * n\n",
        "    # 비복원 추출. 데이터가 적기 때문에 겹치지 않는 게 나을 듯.\n",
        "    X_batch_list.append(X[i:i+n]) # index: 0~4, 4~8, 8~12, ...\n",
        "    y_batch_list.append(y[i:i+n])\n",
        "  return X_batch_list, y_batch_list\n",
        "\n",
        "def train_test_mini_batch(X_train, y_train, X_test, y_test, n):\n",
        "  train_X_batch_list, train_y_batch_list = split_mini_batch(X_train, y_train, n) # train 데이터\n",
        "  test_X_batch_list, test_y_batch_list = split_mini_batch(X_test, y_test, n) # test 데이터\n",
        "  return train_X_batch_list, train_y_batch_list, test_X_batch_list, test_y_batch_list"
      ],
      "metadata": {
        "id": "0NhRSsqyIoaS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Weight and Bias Initialization**"
      ],
      "metadata": {
        "id": "fLPARQmRugzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xavier 초기화(활성화함수가 시그모이드일 때 잘 동작), 편향 포함한 후에 분리하기\n",
        "# 이전 층 노드 개수가 n, 현재 층 노드 개수 m일 때, 표준편차가 2/루트(n+m)인 정규분포로 초기화\n",
        "def initialize_parameter(n, m):\n",
        "  init = np.random.normal(0, 2/((n+m)**2), (n+1, m))\n",
        "  W = init[:-1, :]\n",
        "  b = init[-1, :]\n",
        "  return W, b\n",
        "\n",
        "# initialize_parameter(4, 5)"
      ],
      "metadata": {
        "id": "FVqeboILdV0B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 +np.exp(-x))\n",
        "\n",
        "def classification(x):\n",
        "  if x < 0.5:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "5JHGJwwgBCLL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sequential Layers**"
      ],
      "metadata": {
        "id": "jwpSoZNzuvrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer():  # 입력층, 은닉층, 출력층\n",
        "  def __init__(self, node_num, activation='linear'):\n",
        "    self.node_num = node_num  # 레이어의 노드 개수\n",
        "    self.activation = activation  # 활성화 함수\n",
        "    self.prev = None  # 이전 층\n",
        "    self.next = None  # 다음 층\n",
        "\n",
        "  def set_weights(self):  # 가중치 행렬, 편향 초기화\n",
        "    if self.prev is not None:\n",
        "      prev_node_num = self.prev.node_num\n",
        "      self.W, self.b = initialize_parameter(prev_node_num, self.node_num)\n",
        "  \n",
        "  # input 행렬 X\n",
        "  @property\n",
        "  def X(self):\n",
        "    return self._X\n",
        "  @X.setter\n",
        "  def X(self, value):\n",
        "    self._X = value\n",
        "\n",
        "class Dense(Layer): # 은닉층, 출력층\n",
        "  def __init__(self, node_num, activation='linear'):\n",
        "    # super().__init__(self, node_num)\n",
        "    self.node_num = node_num\n",
        "    self.activation = activation\n",
        "    self.prev = None\n",
        "    self.next = None\n",
        "  \n",
        "  def output(self):\n",
        "    answer = np.dot(self._X, self.W) + self.b\n",
        "    if self.activation == 'linear': # 활성화함수 없음(선형함수)\n",
        "      return answer\n",
        "    elif self.activation == 'sigmoid': # 활성화함수 : 시그모이드\n",
        "      answer = sigmoid(answer)\n",
        "      return answer\n",
        "\n",
        "class Input(Layer): # 입력층\n",
        "  def __init__(self, node_num, activation='linear'):\n",
        "    self.node_num = node_num\n",
        "    self.activation = activation\n",
        "    self.prev = None\n",
        "    self.next = None\n",
        "  # 입력층의 경우 input을 그대로 출력한다\n",
        "  def output(self): \n",
        "    return self._X"
      ],
      "metadata": {
        "id": "6OZ0YaOUka_n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential([])에 layer 쌓고 서로 연결되도록 하기. 가중치 초기화 가능해야 함\n",
        "class Sequential():\n",
        "  def __init__(self, layer_list): # Layer들을 서로 링크드리스트로 연결. 처음과 끝 지정.\n",
        "    # layer가 없는 경우\n",
        "    if len(layer_list)==0:\n",
        "      self.head = None\n",
        "      self.tail = None\n",
        "    # layer가 1개인 경우\n",
        "    elif len(layer_list)==1:\n",
        "      self.head = layer_list[0]\n",
        "      self.tail = layer_list[0]\n",
        "    else: # layer가 2개 이상인 경우\n",
        "      self.head = layer_list[0]\n",
        "      iterator = self.head\n",
        "      for layer in layer_list[1:-1]:\n",
        "        layer.prev= iterator\n",
        "        iterator.next = layer\n",
        "        iterator = layer\n",
        "      iterator.next = layer_list[-1]\n",
        "      self.tail = layer_list[-1]\n",
        "      self.tail.prev = iterator\n",
        "\n",
        "    # 가중치, 편향 초기화\n",
        "    iterator = self.head\n",
        "    while iterator:\n",
        "      iterator.set_weights()\n",
        "      iterator = iterator.next\n",
        "\n",
        "  # input 행렬 X\n",
        "  @property\n",
        "  def input(self):\n",
        "    return self._input\n",
        "  @input.setter\n",
        "  def input(self, value):\n",
        "    self._input = value\n",
        "    self.head.X = value # 처음 층 input에도 들어가도록 하기\n",
        "\n",
        "  def output(self):\n",
        "    iterator = self.head  # Input\n",
        "    while iterator.next:\n",
        "      iterator.next.X = iterator.output()\n",
        "      iterator = iterator.next\n",
        "    return iterator.output()\n",
        "    "
      ],
      "metadata": {
        "id": "jVSnp3_GoODD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential([Input(8), Dense(16), Dense(32), Dense(1, activation='sigmoid')])"
      ],
      "metadata": {
        "id": "3QmP3JdVbBzY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict and Evaluate**"
      ],
      "metadata": {
        "id": "aCOSJWptu1im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(model, y_pred_prob, y_batch, learning_rate=1):\n",
        "    def sigmoid_prime(y):\n",
        "        return y * (1 - y)\n",
        "    # input(X), output(predict), y, error(cross_entropy)\n",
        "    def cross_entropy_prime(y_pred_prob, y_batch):  # binary cross entropy\n",
        "        return y_pred_prob - y_batch\n",
        "\n",
        "    iterator = model.tail\n",
        "    while iterator != model.head:\n",
        "        # dError/dY\n",
        "        if iterator == model.tail:\n",
        "            error = cross_entropy_prime(y_pred_prob, y_batch)\n",
        "        else:\n",
        "            error = np.dot(delta, iterator.next.W.T)    # (체크)\n",
        "        # dY/dy\n",
        "        if iterator.activation == 'sigmoid':\n",
        "            delta = error * sigmoid_prime(y_pred_prob)\n",
        "        elif iterator.activation == 'linear':\n",
        "            delta = error\n",
        "        # dy/dw\n",
        "        delta_mean = np.mean(delta)\n",
        "        iterator.b -= learning_rate * delta_mean\n",
        "        iterator.W -= learning_rate * np.dot(iterator.X.T, delta)\n",
        "        iterator = iterator.prev"
      ],
      "metadata": {
        "id": "6kzhqeCPCXrz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_batch, y_batch, model):\n",
        "    model.input = X_batch\n",
        "    y_pred_prob = model.output()\n",
        "    return y_pred_prob\n",
        "\n",
        "def evaluate_mini_batch(X_batch, y_batch, model, i):\n",
        "    def accuracy(y_pred, y_batch):  # 정확도\n",
        "        return sum(y_pred == y_batch)[0] / y_pred.shape[0]\n",
        "\n",
        "    def cross_entropy(y_pred_prob, y_batch):    # 손실함수\n",
        "        delta = 1e-7\n",
        "        return -np.mean(y_batch * np.log(y_pred_prob + delta) + (1 - y_batch) * np.log(1 - y_pred_prob + delta))   # (체크)\n",
        "\n",
        "    def classification(x):\n",
        "        if x < 0.5:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "        # return 0 if x < 0.5 else 1\n",
        "        # return round(x, 0)\n",
        "\n",
        "    classify = np.vectorize(classification)\n",
        "    y_pred_prob = predict(X_batch, y_batch, model)\n",
        "    y_pred = classify(y_pred_prob)\n",
        "    accuracy = accuracy(y_pred, y_batch)\n",
        "    cross_entropy = cross_entropy(y_pred_prob, y_batch)\n",
        "    print(f\"[Mini-Batch {i+1}] Loss = {round(cross_entropy, 3)}, Accuracy = {round(accuracy, 3)}\")\n",
        "    back_prop(model, y_pred_prob, y_batch)\n",
        "    return accuracy, cross_entropy"
      ],
      "metadata": {
        "id": "CK9B-SGXGJGm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_epoch(X_batch_list, y_batch_list, model, epoch):\n",
        "    accuracy_list = []\n",
        "    cross_entropy_list = []\n",
        "    for i in range(len(X_batch_list)):\n",
        "        X_batch = X_batch_list[i]\n",
        "        y_batch = y_batch_list[i]\n",
        "        accuracy, cross_entropy = evaluate_mini_batch(X_batch, y_batch, model, i)\n",
        "        accuracy_list.append(accuracy)\n",
        "        cross_entropy_list.append(cross_entropy)\n",
        "\n",
        "    total_accuracy = np.mean(accuracy_list)\n",
        "    total_cross_entropy = np.mean(cross_entropy_list)\n",
        "    print(f\"[Epoch {epoch+1}] Loss = {round(total_cross_entropy, 3)}, Accuracy = {round(total_accuracy, 3)}\\n\")"
      ],
      "metadata": {
        "id": "JtqJKtRpCQUM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "2P2zb8-uu47c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # csv 데이터 불러오기\n",
        "    df = data_load()\n",
        "\n",
        "    # train, test 데이터로 나누기\n",
        "    X_train, y_train, X_test, y_test = train_test_split(df)\n",
        "    # 미니배치(하나에 4개 데이터)로 나누기\n",
        "    X_batch_list, y_batch_list, test_X_batch_list, test_y_batch_list = train_test_mini_batch(X_train, y_train, X_test, y_test, 4)\n",
        "\n",
        "    # 모델 만들기(입력층, 은닉층, 출력층)\n",
        "    model = Sequential([Input(8), Dense(8), Dense(1, activation='sigmoid')])\n",
        "\n",
        "    # 모델 실행 및 정확도, 손실함수 측정\n",
        "    print(\"<Train data>\")\n",
        "    for i in range(10):\n",
        "        evaluate_epoch(X_batch_list, y_batch_list, model, i)\n",
        "    # evaluate_epoch(X_batch_list, y_batch_list, model, 0)    # epoch 1\n",
        "    print(\"<Test data>\")\n",
        "    evaluate_epoch(test_X_batch_list, test_y_batch_list, model, 0)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "Ms8-SYlgCbzz",
        "outputId": "d2dbc4ba-4202-4665-9816-456829f6c4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Train data>\n",
            "[Mini-Batch 1] Loss = 0.697, Accuracy = 0.25\n",
            "[Mini-Batch 2] Loss = 0.667, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.696, Accuracy = 0.5\n",
            "[Mini-Batch 4] Loss = 0.607, Accuracy = 1.0\n",
            "[Epoch 1] Loss = 0.667, Accuracy = 0.688\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.613, Accuracy = 0.75\n",
            "[Mini-Batch 2] Loss = 0.485, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.721, Accuracy = 0.5\n",
            "[Mini-Batch 4] Loss = 0.406, Accuracy = 1.0\n",
            "[Epoch 2] Loss = 0.556, Accuracy = 0.812\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.425, Accuracy = 0.75\n",
            "[Mini-Batch 2] Loss = 0.197, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.787, Accuracy = 0.5\n",
            "[Mini-Batch 4] Loss = 0.212, Accuracy = 1.0\n",
            "[Epoch 3] Loss = 0.405, Accuracy = 0.812\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.227, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.096, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.806, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.143, Accuracy = 1.0\n",
            "[Epoch 4] Loss = 0.318, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.122, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.067, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.839, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.114, Accuracy = 1.0\n",
            "[Epoch 5] Loss = 0.286, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.086, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.055, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.866, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.099, Accuracy = 1.0\n",
            "[Epoch 6] Loss = 0.277, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.07, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.048, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.886, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.09, Accuracy = 1.0\n",
            "[Epoch 7] Loss = 0.273, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.061, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.044, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.9, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.083, Accuracy = 1.0\n",
            "[Epoch 8] Loss = 0.272, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.054, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.041, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.91, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.079, Accuracy = 1.0\n",
            "[Epoch 9] Loss = 0.271, Accuracy = 0.938\n",
            "\n",
            "[Mini-Batch 1] Loss = 0.05, Accuracy = 1.0\n",
            "[Mini-Batch 2] Loss = 0.039, Accuracy = 1.0\n",
            "[Mini-Batch 3] Loss = 0.917, Accuracy = 0.75\n",
            "[Mini-Batch 4] Loss = 0.076, Accuracy = 1.0\n",
            "[Epoch 10] Loss = 0.27, Accuracy = 0.938\n",
            "\n",
            "<Test data>\n",
            "[Mini-Batch 1] Loss = 1.541, Accuracy = 0.25\n",
            "[Epoch 1] Loss = 1.541, Accuracy = 0.25\n",
            "\n"
          ]
        }
      ]
    }
  ]
}